


Contents
Top	1
[Feature Super-Resolution: Make Machine See More Clearly (2018)	1]
[High-Quality Face Image Super-Resolution Using Conditional Generative Adversarial Networks (Jul 2017)	5]
[Image Super-Resolution Using Deep Convolutional Networks (Jul 2015)	7]
[Accurate Image Super-Resolution Using Very Deep Convolutional Networks (2016)	9]
[Deeply-Recursive Convolutional Network for Image Super-Resolution (Nov 2016)	11]
[Photo-Realistic Single Image Super-Resolution Using a Generative Adversarial Network (May 2017)	13]
[OUR-GAN: One-shot Ultra-high-Resolution Generative Adversarial Networks (Apr 2022)	15]
[TGAN: Deep Tensor Generative Adversarial Nets for Large Image Generation (May 2019)	17]
[Bottom	20]


Top

### Jun 2014 - Generative Adversarial Nets
<br>**Abstract**</br>
We propose a new framework for estimating [[gans]][1] generative models via an adversarial process, in which we simultaneously train two models: a generative model G that captures the data distribution, and a discriminative model D that estimates the probability that a sample came from the training data rather than G. The training procedure for G is to maximize the probability of D making a mistake. This framework corresponds to a minimax two-player game. In the space of arbitrary functions G and D, a unique solution exists, with G recovering the training data distribution and D equal to 1 2 everywhere. In the case where G and D are defined by multilayer perceptrons, the entire system can be trained with backpropagation. There is no need for any Markov chains or unrolled approximate inference networks during either training or generation of samples. Experiments demonstrate the potential of the framework through qualitative and quantitative evaluation of the generated samples.
<br>**Introduction** </br>
The promise of deep learning is to discover rich, hierarchical models [2] that represent probability distributions over the kinds of data encountered in artificial intelligence applications, such as natural images, audio waveforms containing speech, and symbols in natural language corpora. So far, the most striking successes in deep learning have involved discriminative models, usually those that map a high-dimensional, rich sensory input to a class label [14, 22]. These striking successes have primarily been based on the backpropagation and dropout algorithms, using piecewise linear units [19, 9, 10] which have a particularly well-behaved gradient . Deep generative models have had less of an impact, due to the difficulty of approximating many intractable probabilistic computations that arise in maximum likelihood estimation and related strategies, and due to difficulty of leveraging the benefits of piecewise linear units in the generative context. We propose a new generative model estimation procedure that sidesteps these difficulties. 1 In the proposed adversarial nets framework, the generative model is pitted against an adversary: a discriminative model that learns to determine whether a sample is from the model distribution or the data distribution. The generative model can be thought of as analogous to a team of counterfeiters, trying to produce fake currency and use it without detection, while the discriminative model is analogous to the police, trying to detect the counterfeit currency. Competition in this game drives both teams to improve their methods until the counterfeits are indistiguishable from the genuine articles.
This framework can yield specific training algorithms for many kinds of model and optimization algorithm. In this article, we explore the special case when the generative model generates samples by passing random noise through a multilayer perceptron, and the discriminative model is also a multilayer perceptron. We refer to this special case as adversarial nets. In this case, we can train both models using only the highly successful backpropagation and dropout algorithms [17] and sample from the generative model using only forward propagation. No approximate inference or Markov chains are necessary
<br>**Conclusions and future work** </br>
This framework admits many straightforward extensions: 
1. A conditional generative model p(x | c) can be obtained by adding c as input to both G and D. 
2. Learned approximate inference can be performed by training an auxiliary network to predict z given x. This is similar to the inference net trained by the wake-sleep algorithm [15] but with the advantage that the inference net may be trained for a fixed generator net after the generator net has finished training. 
3. One can approximately model all conditionals p(xS | x6S) where S is a subset of the indices of x by training a family of conditional models that share parameters. Essentially, one can use adversarial nets to implement a stochastic extension of the deterministic MP-DBM [11]. 
4. Semi-supervised learning: features from the discriminator or inference net could improve performance of classifiers when limited labeled data is available. 
5. Efficiency improvements: training could be accelerated greatly by divising better methods for coordinating G and D or determining better distributions to sample z from during training. This paper has demonstrated the viability of the adversarial modeling framework, suggesting that these research directions could prove useful.
https://arxiv.org/pdf/1406.2661.pdf

**Jul 2015 - Image Super-Resolution Using Deep Convolutional Networks** 
<br>**Abstract**</br>
We propose a deep learning method for single image super-resolution (SR). Our method directly learns an end-to-end mapping between the low/high-resolution images. The mapping is represented as a deep convolutional neural network (CNN) that takes the low-resolution image as the input and outputs the high-resolution one. We further show that traditional sparse-coding-based SR methods can also be viewed as a deep convolutional network. But unlike traditional methods that handle each component separately, our method jointly optimizes all layers. Our deep CNN has a lightweight structure, yet demonstrates state-of-the-art restoration quality, and achieves fast speed for practical on-line usage. We explore different network structures and parameter settings to achieve tradeoffs between performance and speed. Moreover, we extend our network to cope with three color channels simultaneously, and show better overall reconstruction quality.
<br>**Introduction** </br> 
Single image super-resolution (SR) [20], which aims at recovering a high-resolution image from a single lowresolution image, is a classical problem in computer vision. This problem is inherently ill-posed since a multiplicity of solutions exist for any given low-resolution pixel. In other words, it is an underdetermined inverse problem, of which solution is not unique. Such a problem is typically mitigated by constraining the solution space by strong prior information. To learn the prior, recent state-of-the-art methods mostly adopt the example-based [46] strategy. These methods either exploit internal similarities of the same image [5], [13], [16], [19], [47], or learn mapping functions from external low- and high-resolution exemplar pairs [2], [4], [6], [15], [23], [25], [37], [41], [42], [47], [48], [50], [51]. The external example-based methods can be formulated for generic image super-resolution, or can be designed to suit domain specific tasks, i.e., face hallucination [30], [50], according to the training samples provided. The sparse-coding-based method [49], [50] is one of the representative external example-based SR methods. This method involves several steps in its solution pipeline. First, overlapping patches are densely cropped from the input image and pre-processed (e.g.,subtracting mean and normalization). These patches are then encoded by a low-resolution dictionary. The sparse coefficients are passed into a high-resolution dictionary for reconstructing high-resolution patches. The overlapping reconstructed patches are aggregated (e.g., by weighted averaging) to produce the final output. This pipeline is shared by most external example-based methods, which pay particular attention to learning and optimizing the dictionaries [2], [49], [50] or building efficient mapping functions [25], [41], [42], [47]. However, the rest of the steps in the pipeline have been rarely optimized or considered in an unified optimization framework. In this paper, we show that the aforementioned pipeline is equivalent to a deep convolutional neural network [27] (more details in Section 3.2). Motivated by this fact, we consider a convolutional neural network that directly learns an end-to-end mapping between low- and high-resolution images. Our method differs fundamentally from existing external example-based approaches, in that ours does not explicitly learn the dictionaries [41], [49], [50] or manifolds [2], [4] for modeling the patch space. These are implicitly achieved via hidden layers. Furthermore, the patch extraction and aggregation are also formulated as convolutional layers, so are involved in the optimization. In our method, the entire SR pipeline is fully obtained through learning, with little pre/postprocessing. We name the proposed model Super-Resolution Convolutional Neural Network (SRCNN)1 . The proposed SRCNN has several appealing properties. First, its structure is intentionally designed with simplicity in mind, and yet provides superior accuracy2 compared with state-of-the-art example-based methods. Figure 1 shows a comparison on an example. Second, with moderate numbers of filters and layers, our method achieves fast speed for practical on-line usage even on a CPU. Our method is faster than a number of example-based methods, because it is fully feed-forward and does not need to solve any optimization problem on usage. Third, experiments show that the restoration quality of the network can be further improved when (i) larger and more diverse datasets are available, and/or (ii) a larger and deeper model is used. On the contrary, larger datasets/models can present challenges for existing example-based methods. Furthermore, the proposed network can cope with three channels of color images simultaneously to achieve improved super-resolution performance. Overall, the contributions of this study are mainly in three aspects: 1) We present a fully convolutional neural network for image super-resolution. The network directly learns an end-to-end mapping between lowand high-resolution images, with little pre/postprocessing beyond the optimization. 2) We establish a relationship between our deep learning-based SR method and the traditional sparse-coding-based SR methods. This relationship provides a guidance for the design of the network structure. 3) We demonstrate that deep learning is useful in the classical computer vision problem of superresolution, and can achieve good quality and speed. A preliminary version of this work was presented earlier [11]. The present work adds to the initial version in significant ways. Firstly, we improve the SRCNN by introducing larger filter size in the non-linear mapping layer, and explore deeper structures by adding nonlinear mapping layers. Secondly, we extend the SRCNN to process three color channels (either in YCbCr or RGB color space) simultaneously. Experimentally, we demonstrate that performance can be improved in comparison to the single-channel network. Thirdly, considerable new analyses and intuitive explanations are added to the initial results. We also extend the original experiments from Set5 [2] and Set14 [51] test images to BSD200 [32] (200 test images). In addition, we compare with a number of recently published methods and confirm that our model still outperforms existing approaches using different evaluation metrics.

<br>**Conclusions and future work** </br>
We have presented a novel deep learning approach for single image super-resolution (SR). We show that conventional sparse-coding-based SR methods can be reformulated into a deep convolutional neural network. The proposed approach, SRCNN, learns an end-to-end mapping between low- and high-resolution images, with little extra pre/post-processing beyond the optimization. With a lightweight structure, the SRCNN has achieved superior performance than the state-of-the-art methods. We conjecture that additional performance can be further gained by exploring more filters and different training strategies. Besides, the proposed structure, with its advantages of simplicity and robustness, could be applied to other low-level vision problems, such as image deblurring or simultaneous SR+denoising. One could also investigate a network to cope with different upscaling factors.

https://arxiv.org/pdf/1501.00092.pdf

 
  **Jan 2016 - UNSUPERVISED REPRESENTATION LEARNING WITH DEEP CONVOLUTIONAL GENERATIVE ADVERSARIAL NETWORKS**
<br>**Abstract**</br> 
In recent years, supervised learning with convolutional networks (CNNs) has seen huge adoption in computer vision applications. Comparatively, unsupervised learning with CNNs has received less attention. In this work we hope to help bridge the gap between the success of CNNs for supervised learning and unsupervised learning. We introduce a class of CNNs called deep convolutional generative adversarial networks (DCGANs), that have certain architectural constraints, and demonstrate that they are a strong candidate for unsupervised learning. Training on various image datasets, we show convincing evidence that our deep convolutional adversarial pair learns a hierarchy of representations from object parts to scenes in both the generator and discriminator. Additionally, we use the learned features for novel tasks - demonstrating their applicability as general image representations.
<br>**Introduction** </br> 
Learning reusable feature representations from large unlabeled datasets has been an area of active research. In the context of computer vision, one can leverage the practically unlimited amount of unlabeled images and videos to learn good intermediate representations, which can then be used on a variety of supervised learning tasks such as image classification. We propose that one way to build good image representations is by training Generative Adversarial Networks (GANs) (Goodfellow et al., 2014), and later reusing parts of the generator and discriminator networks as feature extractors for supervised tasks. GANs provide an attractive alternative to maximum likelihood techniques. One can additionally argue that their learning process and the lack of a heuristic cost function (such as pixel-wise independent mean-square error) are attractive to representation learning. GANs have been known to be unstable to train, often resulting in generators that produce nonsensical outputs. There has been very limited published research in trying to understand and visualize what GANs learn, and the intermediate representations of multi-layer GANs. In this paper, we make the following contributions:
• We propose and evaluate a set of constraints on the architectural topology of Convolutional GANs that make them stable to train in most settings. We name this class of architectures Deep Convolutional GANs (DCGAN) 
• We use the trained discriminators for image classification tasks, showing competitive performance with other unsupervised algorithms. 
• We visualize the filters learnt by GANs and empirically show that specific filters have learned to draw specific objects. 
• We show that the generators have interesting vector arithmetic properties allowing for easy manipulation of many semantic qualities of generated samples.

<br>**Conclusions and future work** </br> 
We propose a more stable set of architectures for training generative adversarial networks and we give evidence that adversarial networks learn good representations of images for supervised learning and generative modeling. There are still some forms of model instability remaining - we noticed as models are trained longer they sometimes collapse a subset of filters to a single oscillating mode.
Further work is needed to tackle this from of instability. We think that extending this framework to other domains such as video (for frame prediction) and audio (pre-trained features for speech synthesis) should be very interesting. Further investigations into the properties of the learnt latent space would be interesting as well.
https://arxiv.org/pdf/1511.06434.pdf

 
**2016 - Accurate Image Super-Resolution Using Very Deep Convolutional Networks** 
<br>**Abstract**</br> 
We present a highly accurate single-image superresolution (SR) method. Our method uses a very deep convolutional network inspired by VGG-net used for ImageNet classification [19]. We find increasing our network depth shows a significant improvement in accuracy. Our final model uses 20 weight layers. By cascading small filters many times in a deep network structure, contextual information over large image regions is exploited in an efficient way. With very deep networks, however, convergence speed becomes a critical issue during training. We propose a simple yet effective training procedure. We learn residuals only and use extremely high learning rates (104 times higher than SRCNN [6]) enabled by adjustable gradient clipping. Our proposed method performs better than existing methods in accuracy and visual improvements in our results are easily noticeable.
<br>**Introduction** </br> 
We address the problem of generating a high-resolution (HR) image given a low-resolution (LR) image, commonly referred as single image super-resolution (SISR) [12], [8], [9]. SISR is widely used in computer vision applications ranging from security and surveillance imaging to medical imaging where more image details are required on demand. Many SISR methods have been studied in the computer vision community. Early methods include interpolation such as bicubic interpolation and Lanczos resampling [7] more powerful methods utilizing statistical image priors [20, 13] or internal patch recurrence [9]. Currently, learning methods are widely used to model a mapping from LR to HR patches. Neighbor embedding [4, 15] methods interpolate the patch subspace. Sparse coding [25, 26, 21, 22] methods use a learned compact dictionary based on sparse signal representation. Lately, random forest [18] and convolutional neural network (CNN) [6] have also been used with large improvements in accuracy. Among them, Dong et al. [6] has demonstrated that a CNN can be used to learn a mapping from LR to HR in an end-to-end manner. Their method, termed SRCNN, does not require any engineered features that are typically necessary in other methods [25, 26, 21, 22] and shows the stateof-the-art performance. While SRCNN successfully introduced a deep learning technique into the super-resolution (SR) problem, we find its limitations in three aspects: first, it relies on the context of small image regions; second, training converges too slowly; third, the network only works for a single scale. In this work, we propose a new method to practically resolve the issues. Context We utilize contextual information spread over very large image regions. For a large scale factor, it is often the case that information contained in a small patch is not sufficient for detail recovery (ill-posed). Our very deep network using large receptive field takes a large image context into account. Convergence We suggest a way to speed-up the training: residual-learning CNN and extremely high learning rates. As LR image and HR image share the same information to a large extent, explicitly modelling the residual image, which is the difference between HR and LR images, is advantageous. We propose a network structure for effcient learning when input and output are highly correlated. Moreover, our initial learning rate is 104 times higher than that of SRCNN [6]. This is enabled by residual-learning and gradient clipping. Scale Factor We propose a single-model SR approach. Scales are typically user-specified and can be arbitrary including fractions. For example, one might need smooth zoom-in in an image viewer or resizing to a specific dimension. Training and storing many scale-dependent models in preparation for all possible scenarios is impractical. We find a single convolutional network is sufficient for multi-scalefactor super-resolution. Contribution In summary, in this work, we propose a highly accurate SR method based on a very deep convolutional network. Very deep networks converge too slowly if small learning rates are used. Boosting convergence rate with high learning rates lead to exploding gradients and we resolve the issue with residual-learning and gradient clipping. In addition, we extend our work to cope with multiscale SR problem in a single network. Our method is relatively accurate and fast in comparison to state-of-the-art methods as illustrated in Figure 1.
<br>**Conclusions and future work** </br>
In this work, we have presented a super-resolution method using very deep networks. Training a very deep network is hard due to a slow convergence rate. We use residual-learning and extremely high learning rates to optimize a very deep network fast. Convergence speed is maximized and we use gradient clipping to ensure the training stability. We have demonstrated that our method outperforms the existing method by a large margin on benchmarked images. We believe our approach is readily applicable to other image restoration problems such as denoising and compression artifact removal.

https://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&arnumber=7780551


 
**Nov 2016 - Deeply-Recursive Convolutional Network for Image Super-Resolution** 
<br>**Abstract**</br> 
We propose an image super-resolution method (SR) using a deeply-recursive convolutional network (DRCN). Our network has a very deep recursive layer (up to 16 recursions). Increasing recursion depth can improve performance without introducing new parameters for additional convolutions. Albeit advantages, learning a DRCN is very hard with a standard gradient descent method due to exploding/vanishing gradients. To ease the difficulty of training, we propose two extensions: recursive-supervision and skip-connection. Our method outperforms previous methods by a large margin.
<br>**Introduction** </br> 
For image super-resolution (SR), receptive field of a convolutional network determines the amount of contextual information that can be exploited to infer missing highfrequency components. For example, if there exists a pattern with smoothed edges contained in a receptive field, it is plausible that the pattern is recognized and edges are appropriately sharpened. As SR is an ill-posed inverse problem, collecting and analyzing more neighbor pixels can possibly give more clues on what may be lost by downsampling. Deep convolutional networks (DCN) succeeding in various computer vision tasks often use very large receptive fields (224x224 common in ImageNet classification [13, 24]). Among many approaches to widen the receptive field, increasing network depth is one possible way: a convolutional (conv.) layer with filter size larger than a 1×1 or a pooling (pool.) layer that reduces the dimension of intermediate representation can be used. Both approaches have drawbacks: a conv. layer introduces more parameters and a pool. layer typically discards some pixel-wise information. For image restoration problems such as super-resolution and denoising, image details are very important. Therefore, most deep-learning approaches for such problems do not use pooling. Increasing depth by adding a new weight layer basically introduces more parameters. Two problems can arise. First, overfitting is highly likely. More data are now required. Second, the model becomes too huge to be stored and retrieved. To resolve these issues, we use a deeply-recursive convolutional network (DRCN). DRCN repeatedly applies the same convolutional layer as many times as desired. The number of parameters do not increase while more recursions are performed. Our network has the receptive field of 41 by 41 and this is relatively large compared to SRCNN [5] (13 by 13). While DRCN has good properties, we find that DRCN optimized with the widely-used stochastic gradient descent method does not easily converge. This is due to exploding/vanishing gradients [1]. Learning long-range dependencies between pixels with a single weight layer is very difficult. We propose two approaches to ease the difficulty of training (Figure 3(a)). First, all recursions are supervised. Feature maps after each recursion are used to reconstruct the target high-resolution image (HR). Reconstruction method (layers dedicated to reconstruction) is the same for all recursions. As each recursion leads to a different HR prediction, we combine all predictions resulting from different levels of recursions to deliver a more accurate final prediction. The second proposal is to use a skip-connection from input to the reconstruction layer. In SR, a low-resolution image (input) and a high-resolution image (output) share the same information to a large extent. Exact copy of input, however, is likely to be attenuated during many forward passes. We explicitly connect the input to the layers for output reconstruction. This is particularly effective when input and output are highly correlated.
<br>**Conclusions and future work** </br>
In this work, we have presented a super-resolution method using a deeply-recursive convolutional network. Our network efficiently reuses weight parameters while exploiting a large image context. To ease the difficulty of training the model, we use recursive-supervision and skipconnection. We have demonstrated that our method outperforms existing methods by a large margin on benchmarked images. In the future, one can try more recursions in order to use image-level context. We believe our approach is readily applicable to other image restoration problems such as denoising and compression artifact removal.
https://arxiv.org/pdf/1511.04491.pdf

 
**May 2017 - Photo-Realistic Single Image Super-Resolution Using a Generative Adversarial Network** 
<br>**Abstract**</br> 
Despite the breakthroughs in accuracy and speed of single image super-resolution using faster and deeper convolutional neural networks, one central problem remains largely unsolved: how do we recover the finer texture details when we super-resolve at large upscaling factors? The behavior of optimization-based super-resolution methods is principally driven by the choice of the objective function. Recent work has largely focused on minimizing the mean squared reconstruction error. The resulting estimates have high peak signal-to-noise ratios, but they are often lacking high-frequency details and are perceptually unsatisfying in the sense that they fail to match the fidelity expected at the higher resolution. In this paper, we present SRGAN, a generative adversarial network (GAN) for image superresolution (SR). To our knowledge, it is the first framework capable of inferring photo-realistic natural images for 4× upscaling factors. To achieve this, we propose a perceptual loss function which consists of an adversarial loss and a content loss. The adversarial loss pushes our solution to the natural image manifold using a discriminator network that is trained to differentiate between the super-resolved images and original photo-realistic images. In addition, we use a content loss motivated by perceptual similarity instead of similarity in pixel space. Our deep residual network is able to recover photo-realistic textures from heavily downsampled images on public benchmarks. An extensive mean-opinion-score (MOS) test shows hugely significant gains in perceptual quality using SRGAN. The MOS scores obtained with SRGAN are closer to those of the original high-resolution images than to those obtained with any state-of-the-art method.
<br>**Introduction** </br> 
The highly challenging task of estimating a highresolution (HR) image from its low-resolution (LR) counterpart is referred to as super-resolution (SR). SR received substantial attention from within the computer vision research community and has a wide range of applications [63, 71, 43].  The ill-posed nature of the underdetermined SR problem is particularly pronounced for high upscaling factors, for which texture detail in the reconstructed SR images is typically absent. The optimization target of supervised SR algorithms is commonly the minimization of the mean squared error (MSE) between the recovered HR image and the ground truth. This is convenient as minimizing MSE also maximizes the peak signal-to-noise ratio (PSNR), which is a common measure used to evaluate and compare SR algorithms [61]. However, the ability of MSE (and PSNR) to capture perceptually relevant differences, such as high texture detail, is very limited as they are defined based on pixel-wise image differences [60, 58, 26]. This is illustrated in Figure 2, where highest PSNR does not necessarily reflect the perceptually better SR result. The perceptual difference between the super-resolved and original image means that the recovered image is not photorealistic as defined by Ferwerda [16]. In this work we propose a super-resolution generative adversarial network (SRGAN) for which we employ a deep residual network (ResNet) with skip-connection and diverge from MSE as the sole optimization target. Different from previous works, we define a novel perceptual loss using high-level feature maps of the VGG network [49, 33, 5] combined with a discriminator that encourages solutions perceptually hard to distinguish from the HR reference images. An example photo-realistic image that was superresolved with a 4× upscaling factor is shown in Figure 1.
<br>**Conclusions and future work** </br> 
We have described a deep residual network SRResNet that sets a new state of the art on public benchmark datasets when evaluated with the widely used PSNR measure. We have highlighted some limitations of this PSNR- focused image super-resolution and introduced SRGAN, which augments the content loss function with an adversarial loss by training a GAN. Using extensive MOS testing, we have confirmed that SRGAN reconstructions for large upscaling factors (4×) are, by a considerable margin, more photo-realistic than reconstructions obtained with state-ofthe-art reference methods.

https://arxiv.org/pdf/1609.04802.pdf

 
**Jul 2017 - High-Quality Face Image Super-Resolution Using Conditional Generative Adversarial Networks**
<br>**Abstract**</br>
We propose a novel single face image superresolution method, which named Face Conditional Generative Adversarial Network(FCGAN), based on boundary equilibrium generative adversarial networks. Without taking any facial prior information, our method can generate a high-resolution face image from a low-resolution one. Compared with existing studies, both our training and testing phases are end-toend pipeline with little pre/post-processing. To enhance the convergence speed and strengthen feature propagation, skiplayer connection is further employed in the generative and discriminative networks. Extensive experiments demonstrate that our model achieves competitive performance compared with state-of-the-art models. 
<br>**Introduction** </br> 
Single image super resolution(SISR), a greatly challenging task of computer vision and machine learning, attempts to reconstruct a high-resolution(HR) image from a lowresolution(LR) image. Super resolution(SR) is commonly divided into two categories based on their tasks, namely generic image SR and class-specific image SR. The former takes little class information into account, which aims to recover any kinds of high resolution image from corresponding low-resolution image. In general, the latter usually refers to face image super resolution or face hallucination if the class is face. Face image super resolution or face hallucination[1–9] is an important branch of super-resolution(SR). The great distinction between the both techniques is that face hallucination always employs typical facial priors (eg. face spatial configuration and facial landmark detection) with strong cohesion to face domain concept. More realistic and sharper details, which plays a crucial role in intelligence surveillance[1, 3] and face recognition[9], are taken by HR face images than corresponding LR images. Due to long distance imaging, the limitations on storage and low-cost electronic imaging systems, LR images appear in many cases instead of HR images. Thus, SR has turned out to be an active research filed in the past few years. 
Face image SR is an ill-posed problem (as same as generic image SR), for which it needs to recover 16 pixels (for 4× upscaling factors) from each given pixel. While, recent years have witnessed a tremendous growth of research and development in the field, in particular using learningbased methods.
In this paper, we propose a HR face image framework (4× upscaling factors) based on boundary equilibrium generative adversarial network(BEGAN)[10]. In order to adapt BEGAN for SR task, single low-resolution face image is considered as the prior condition to generate a highresolution one. So, we refer to the framework as Face Conditional Generative Adversarial Network(named FCGAN for short hereafter). Our proposed method does not utilize any priors on face structure or face spatial configuration. In addition, it is also an end-to-end solution to generate HR face images without need any pre-trained model. We perform extensive experiments, which demonstrates that our method not only achieves high Peak Signal to Noise Ratio(PSNR), but also improves actual visual quality. Overall, the contributions of this paper are mainly in three aspects: • We propose a novel end-to-end method (FCGAN), with 4× upscaling factors, to learn mapping between low-resolution single face images to highresolution one. The method can robustly generate a high-quality face image from low-resolution one. • To the best of our knowledge, our method is the first attempt to develop BEGAN[10] to generate HR face images from low-resolution ones regardless of pose, facial expressions variation, face alignment and lighting. Our model considers a low-resolution image I LR as the input instead of random noise. • We introduce the pixel-wise L1 loss function to optimize the generative and discriminative models. Compared with state-of-the-art models, extensive experiments show that FCGAN achieve competitive performance on both visual quality and quantitative analysis.

<br>**Conclusions and future work** </br> 
In this paper, we have proposed a novel SR method (4× upscaling factors) to generate a HR face image from LR one, namely Face Conditional Generative Adversarial Network (FCGAN). In this model, the LR image, instead of random noise, is considered as a controller to generate a HR image. Our FCGAN is an end-to-end framework, without any pre/post-processing (e.g., face alignment, extracting facial structure prior information). Furthermore, it is a robustly model, the generative image is not sensitive to facial expression, pose, illumination, occlusion (wearing glasses or hat), and so on. For the generator and discriminator networks, the skip-layer connection technique is utilized for enhancing the convergence speed in the training phase. Thus, our model has great advantages on the training time over other SR models based on CNN. However, there are several problems that worth to further investigate in the future. We note that the input image size of recent FCGAN model is same as the generative HR image (128×128). In the future research, we will design an advanced model that can directly generate HR face image (e.g., 128 × 128) from the small size one (e.g., 32 × 32). In addition, we only show the excellent performance on face image SR task in this work, and it is worth to extend our proposed framework for the task of generic image SR

https://arxiv.org/pdf/1707.00737.pdf


 
**2018 - Feature Super-Resolution: Make Machine See More Clearly**
<br>**Abstract**</br> </br>
Identifying small size images or small objects is a notoriously challenging problem, as discriminative representations are difficult to learn from the limited information contained in them with poor-quality appearance and unclear object structure. Existing research works usually increase the resolution of low-resolution image in the pixel space in order to provide better visual quality for human viewing. However, the improved performance of such methods is usually limited or even trivial in the case of very small image size (we will show it in this paper explicitly). In this paper, different from image super-resolution (ISR), we propose a novel super-resolution technique called feature super-resolution (FSR), which aims at enhancing the discriminatory power of small size image in order to provide high recognition precision for machine. To achieve this goal, we propose a new Feature Super-Resolution Generative Adversarial Network (FSR-GAN) model that transforms the raw poor features of small size images to highly discriminative ones by performing super-resolution in the feature space. Our FSR-GAN consists of two subnetworks: a feature generator network G and a feature discriminator network D. By training the G and the D networks in an alternative manner, we encourage the G network to discover the latent distribution correlations between small size and large size images and then use G to improve the representations of small images. Extensive experiment results on Oxford5K, Paris, Holidays, and Flick100k datasets demonstrate that the proposed FSR approach can effectively enhance the discriminatory ability of features. Even when the resolution of query images is reduced greatly, e.g., 1/64 original size, the query feature enhanced by our FSR approach achieves surprisingly high retrieval performance at different image resolutions and increases the retrieval precision by 25% compared to the raw query feature.

<br>**Introduction** </br> </br>
The powerful deep learning framework makes numerous great classification models presented, such as VGG16 [18], GoogLeNet [19], ResNet [7], SeNet [8], etc. These models achieve an amazing recognition accuracy on Imagenet dataset [2], even better than human beings do. Actually, they indeed work well on large size images with goodquality appearance and rich object structure. However, they usually fail to identify very small size images since discriminative representations are difficult to learn from their lowresolution and noise representation. Small size images or objects are very common in many real-world scenarios such as small pedestrians in surveillance video, small faces in the crowd, traffic signs, small objects, etc. Small size image recognition is much more challenging than normal image recognition and there are rare good solutions so far. Current research works such as image super-resolution (ISR) focus on increasing the resolution of a given image. Its most common application is to provide better visual quality after resizing a digital image for human viewing, as shown in Fig. 1(b). In recent years, numerous image super-resolution approaches have been proposed to restore high frequency information in order to generate high quality images with rich details, and have achieved great success [21, 3, 10, 11, 12, 5, 20]. This process is referred to pixelspace field enhancement in the literature. Those ISR based approaches are able to recover the object details in small size image and can improve the identification accuracy in some extent. In the experiment, we will compare ISR based approaches with our FSR algorithm and demonstrate their limitations. Intuitively, as shown in Fig. 1, if we perform super-resolution for machine recognition instead of human viewing, the paradigm should change accordingly. Therefore, for machine recognition, we propose feature superresolution for improving the discriminative ability of features, as illustrated in Fig. 1(a). In order to practically explore the necessity and feasibility of feature super-resolution, we conduct several experiments to understand the impact of low-resolution image on deep representations. For these experiments, we evaluate the effect of down-scaling operation on the deep representations extracted by using the popular VGG16 model [18] on Oxford5K dataset [14]. The Oxford5K dataset contains 5,063 images and 55 query images. For convenience, the deep representations mentioned below refers to the neural activations in the 36th layer of VGGnet model [18] if there is no special notification. Firstly, we summarize the experimental results of mean Euclidean distance between the deep features extracted from high-resolution images and their corresponding lowresolution ones. The low-resolution images are obtained by performing uniform down-sampling operation with different scaling ratios. Figure. 2(a) shows the change of deep feature with the decrease of image resolution. From Fig. 2(a) we observe that with the decrease of image resolution, the difference between the deep features extract ed from low-resolution images and high-resolution ones is growing wider. This result implies that decreasing image resolution is capable of impacting deep representation, though the powerful VGG16 model [18] is carefully trained on the large-scale Imagenet dataset [2] and adds some useful training tricks. Furthermore, we conduct image retrieval experiment to understand how low-resolution images affect matching/retrieval accuracy when using deep features. We summarize the experimental results of the mean average precision (mAP) as a function of the down-scaling ratio in Fig. 2(b) and Fig. 2(c). The results demonstrate that with the decrease of the resolution of the image, the retrieval precision is decreased rapidly. This phenomenon is reasonable because the low-resolution image has lost many detail information, which results in failing to extract discriminative features even using the powerful very deep convolutional neural network. From Fig. 2(a) to Fig. 2(c) we have known that the low-resolution images not only impact the extracted deep features, but also seriously decrease the matching/retrieval accuracy. To find a solution to the problem, we conduct the third experiment to further understand the relationship between the deep features extracted from different resolution images. Specifically, we calculate the variance of Euclidean distance for the same down-scaling ratio. Table 1 summarizes the experiment result, which reports that the variances of Euclidean distance across different down-scaling ratios are very small. This result means that the extracted deep features are changed regularly with down-scaling ratios, i.e., the change of deep features mainly depends on the amount of information lost instead of specific image content. This key observation provides an important basis for our FSR approach to solve the problem mentioned above. Based on the key observation mentioned above, we propose a novel Feature Super-Resolution Generative Adversarial Network (FSR-GAN) model to enhance the discriminatory ability of the representation of small size images, achieving similar attributes as images with clear appearance and thus more discriminative for better identification. Our FSR-GAN consists of two subnetworks: a feature generator network G and a feature discriminator network D. The G is a simple convolution neural network which maps the raw poor representations of small size images to highly discriminative ones by discovering the latent distribution correlations between small size and large size images, achieving “super-resolution” on the feature space. The D estimates the probability that a representation comes from the real data or the fake data generated by G. It actually provides guidance for updating G. Note that different from traditional Generative Adversarial Network (GAN), our proposed FSR-GAN includes a new focal loss tailored for scale-invariant feature enhancement. In this paper, we propose the FSR concept using the framework of GAN to form a local loss function for representation enhancement. The main contributions of this work are: 
• Based on the key observation, we propose a novel concept of feature super-resolution that is different from the image super resolution. This technique is expected to make a breakthrough in the challenging task of identifying small size images or objects. 
• We are the first to successfully introduce the FSR concept into the GAN framework, called FSR- GAN, achieving large improvement comparing with existing approaches. 
• We introduce a new focal loss for generative network, making it put more effort into hard examples with large downscales and preventing it from being affected by easy examples with small downscales, thus the optimal solution can be obtained. 
• Several successful applications explicitly show that our FSR-GAN is far superior to the comparison approaches.
<br>**Conclusions and future work** </br> 
We have presented a novel feature super-resolution technique for improving the discriminatory power of representations extracted from low-resolution images. By analyzing the impact of down-scaling operation on the deep features, we have two key conclusions. One is that low-resolution images not only impact the extracted deep features, but also seriously decrease the retrieval accuracy. The other is that deep features extracted from low-resolution images are changed regularly with down-scaling ratios, which inspires us to develop a feature super-resolution model to learn the mapping relationship between low-discriminative features and high-discriminative features. Extensive experiment results suggest that our proposed FSR-GAN approach is not only an effective solution for enhancing features, but also shows its great potential in many applications.
https://ieeexplore-ieee-org.ezproxy.is.ed.ac.uk/stamp/stamp.jsp?tp=&arnumber=8578518


 
**Sep 2018 - ESRGAN: Enhanced Super-Resolution Generative Adversarial Networks**
<br>**Abstract**</br>. 
The Super-Resolution Generative Adversarial Network (SRGAN) [1] is a seminal work that is capable of generating realistic textures during single image super-resolution. However, the hallucinated details are often accompanied with unpleasant artifacts. To further enhance the visual quality, we thoroughly study three key components of SRGAN – network architecture, adversarial loss and perceptual loss, and improve each of them to derive an Enhanced SRGAN (ESRGAN). In particular, we introduce the Residual-in-Residual Dense Block (RRDB) without batch normalization as the basic network building unit. Moreover, we borrow the idea from relativistic GAN [2] to let the discriminator predict relative realness instead of the absolute value. Finally, we improve the perceptual loss by using the features before activation, which could provide stronger supervision for brightness consistency and texture recovery. Benefiting from these improvements, the proposed ESRGAN achieves consistently better visual quality with more realistic and natural textures than SRGAN and won the first place in the PIRM2018-SR Challenge1 [3]. The code is available at https://github.com/xinntao/ESRGAN. 
<br>**Introduction** </br> 
Single image super-resolution (SISR), as a fundamental low-level vision problem, has attracted increasing attention in the research community and AI companies. SISR aims at recovering a high-resolution (HR) image from a single low-resolution (LR) one. Since the pioneer work of SRCNN proposed by Dong et al. [4], deep convolution neural network (CNN) approaches have brought prosperous development. Various network architecture designs and training strategies have continuously improved the SR performance, especially the Peak Signal-toNoise Ratio (PSNR) value [5,6,7,1,8,9,10,11,12]. However, these PSNR-oriented approaches tend to output over-smoothed results without sufficient high-frequency details, since the PSNR metric fundamentally disagrees with the subjective evaluation of human observers [1].
Several perceptual-driven methods have been proposed to improve the visual quality of SR results. For instance, perceptual loss [13,14] is proposed to optimize super-resolution model in a feature space instead of pixel space. Generative adversarial network [15] is introduced to SR by [1,16] to encourage the network to favor solutions that look more like natural images. The semantic image prior is further incorporated to improve recovered texture details [17]. One of the milestones in the way pursuing visually pleasing results is SRGAN [1]. The basic model is built with residual blocks [18] and optimized using perceptual loss in a GAN framework. With all these techniques, SRGAN significantly improves the overall visual quality of reconstruction over PSNR-oriented methods. However, there still exists a clear gap between SRGAN results and the ground-truth (GT) images, as shown in Fig. 1. In this study, we revisit the key components of SRGAN and improve the model in three aspects. First, we improve the network structure by introducing the Residual-in-Residual Dense Block (RDDB), which is of higher capacity and easier to train. We also remove Batch Normalization (BN) [19] layers as in [20] and use residual scaling [21,20] and smaller initialization to facilitate training a very deep network. Second, we improve the discriminator using Relativistic average GAN (RaGAN) [2], which learns to judge “whether one image is more realistic than the other” rather than “whether one image is real or fake”. Our experiments show that this improvement helps the generator recover more realistic texture details. Third, we propose an improved perceptual loss by using the VGG features before activation instead of after activation as in SRGAN. We empirically find that the adjusted perceptual loss provides sharper edges and more visually pleasing results, as will be shown in Sec. 4.4. Extensive experiments show that the enhanced SRGAN, termed ESRGAN, consistently outperforms state-of-the-art methods in both sharpness and details (see Fig. 1 and Fig. 7). We take a variant of ESRGAN to participate in the PIRM-SR Challenge [3]. This challenge is the first SR competition that evaluates the performance in a perceptual-quality aware manner based on [22], where the authors claim that distortion and perceptual quality are at odds with each other. The perceptual quality is judged by the non-reference measures of Ma’s score [23] and NIQE [24], i.e., perceptual index = 1 2 ((10−Ma)+NIQE). A lower perceptual index represents a better perceptual quality. As shown in Fig. 2, the perception-distortion plane is divided into three regions defined by thresholds on the Root-Mean-Square Error (RMSE), and the algorithm that achieves the lowest perceptual index in each region becomes the regional champion. We mainly focus on region 3 as we aim to bring the perceptual quality to a new high. Thanks to the aforementioned improvements and some other adjustments as discussed in Sec. 4.6, our proposed ESRGAN won the first place in the PIRM-SR Challenge (region 3) with the best perceptual index. In order to balance the visual quality and RMSE/PSNR, we further propose the network interpolation strategy, which could continuously adjust the reconstruction style and smoothness. Another alternative is image interpolation, which directly interpolates images pixel by pixel. We employ this strategy to participate in region 1 and region 2. The network interpolation and image interpolation strategies and their differences are discussed in Sec. 3.4. 
<br>**Conclusions and future work** </br> 
We have presented an ESRGAN model that achieves consistently better perceptual quality than previous SR methods. The method won the first place in the PIRM-SR Challenge in terms of the perceptual index. We have formulated a novel architecture containing several RDDB blocks without BN layers. In addition, useful techniques including residual scaling and smaller initialization are employed to facilitate the training of the proposed deep model. We have also introduced the use of relativistic GAN as the discriminator, which learns to judge whether one image is more realistic than another, guiding the generator to recover more detailed textures. Moreover, we have enhanced the perceptual loss by using the features before activation, which offer stronger supervision and thus restore more accurate brightness and realistic textures.
https://arxiv.org/pdf/1809.00219.pdf

 
**Nov 2018 - Analyzing Perception-Distortion Tradeoff using Enhanced Perceptual Super-resolution Network**
<br>**Abstract**</br>. 
Convolutional neural network (CNN) based methods have recently achieved great success for image super-resolution (SR). However, most deep CNN based SR models attempt to improve distortion measures (e.g. PSNR, SSIM, IFC, VIF) while resulting in poor quantified perceptual quality (e.g. human opinion score, no-reference quality measures such as NIQE). Few works have attempted to improve the perceptual quality at the cost of performance reduction in distortion measures. A very recent study has revealed that distortion and perceptual quality are at odds with each other and there is always a trade-off between the two. Often the restoration algorithms that are superior in terms of perceptual quality, are inferior in terms of distortion measures. Our work attempts to analyze the trade-off between distortion and perceptual quality for the problem of single image SR. To this end, we use the well-known SR architecture- enhanced deep super-resolution (EDSR) network and show that it can be adapted to achieve better perceptual quality for a specific range of the distortion measure. While the original network of EDSR was trained to minimize the error defined based on perpixel accuracy alone, we train our network using a generative adversarial network framework with EDSR as the generator module. Our proposed network, called enhanced perceptual super-resolution network (EPSR), is trained with a combination of mean squared error loss, perceptual loss, and adversarial loss. Our experiments reveal that EPSR achieves the state-of-the-art trade-off between distortion and perceptual quality while the existing methods perform well in either of these measures alone. Keywords: Super-resolution, deep learning, perceptual quality, GAN.
<br>**Introduction** </br> 
The problem of single image super-resolution (SISR) has attracted much attention and progress in recent years. The primary objective of SISR algorithms is to recover the high-resolution (HR) image from a given single low-resolution (LR) image. By definition, SISR is an ill-posed problem as no unique solution exists for a given LR image. The same LR image can be obtained by down-sampling a large number of different HR images. The ill-posedness of SISR becomes particularly pronounced when the scaling factor increases. Deep learning approaches attempt to solve this ill-posed problem by learning a mapping between the LR and its corresponding HR image in a direct or indirect manner. Recent works on deep neural networks based SISR have shown significant performance improvement in terms of peak signal-to-noise ratio (PSNR). SISR with deep networks gained momentum with the primal work of Chao et al. [12]. While [12] used a 3 layer convolutional neural network (CNN), the subsequent works used deeper network architectures [23,24] and new techniques to improve the restoration accuracy [31,20] and computational complexity [40,13]. Despite significant progress in both reconstruction accuracy and speed, a majority of the existing works are still far away from reconstructing realistic textures. This is mainly because of the fact that these works are aimed at improving distortion scores such as PSNR and structural similarity index (SSIM) by optimizing pixel-wise computed error measures such as mean squared error (MSE). In the context of SISR, the optimal MSE estimator returns the mean of many possible solutions [28,39] which often leads to blurry, overly smooth, and unnatural appearance in the output, especially at the information-rich regions. Previous studies [46,27] revealed that pixel-wise computed error measures correlate poorly with human perception of image quality. Considering the fact that, the behavior of optimization-based SR methods are strongly influenced by the choice of objective function, one should be able to obtain high-quality images by picking the best suited objective function for the task at hand. This is the main motivation behind the recent works on SISR [22,28,39,34] that came up with new ways to improve the perceptual quality of reconstructed images. A detailed analysis conducted by [5] showed that distortion and perceptual quality are at odds with each other and there is always a trade-off between the two. As observed in [5], the restoration algorithms that are superior in terms of perceptual quality, are often inferior in terms of distortion measures. They came up with a new methodology for evaluating image restoration methods which can be used to better reveal this trade-off. They have proposed to map SR methods onto a perception-distortion plane and choose the SR method which yields the lowest perceptual score for a given range of distortion measure as the best performing method for that range. They have also suggested that adversarial loss can be used to achieve the desired trade-off for the specific application in mind. Though the work in [5] concluded that the existing SISR works perform well in either of these metrics, the possibility to achieve better trade-off in different regions of the perception-distortion plane was left unexplored. In this work, we analyze the perception-distortion trade-off that can be achieved by the well-known SISR architecture- enhanced deep super-resolution (EDSR) network [31]. In our analysis, we limit our focus to SISR by a factor of 4 for LR images distorted by the bicubic down-sampling operator. Selection of EDSR was motivated by the fact that it is one of the state-of-the-art network architecture in terms of the distortion measure for SISR. Since the original work of EDSR proposed in [31] is aimed at improving distortion measure alone, the perceptual quality achieved by EDSR is poor as pointed out by [5]. We train EDSR network using a combination of loss functions that can improve distortion measures as well as perceptual quality. Motivated by the observations in [22,28,39,5], we use a combination of MSE loss, perceptual (VGG) loss, and adversarial loss to train EDSR. Use of adversarial loss to improve perceptual quality allowed our approach to traverse different regions in the perception-distortion plane with ease. We name our approach as enhanced perceptual super-resolution network (EPSR). Our experiments reveal that EPSR can be used to achieve the state-of-the-art trade-off between distortion measure and perceptual quality corresponding to three different regions in the perception-distortion plane. Our main contributions are summarized below. 
• We expand the scope of EDSR and show that it can be adapted to improve the perceptual quality by compromising on distortion measures. 
• Our proposed approach achieves the state-of-the-art perception-distortion tradeoff results corresponding to different regions in the perception-distortion plane.
<br>**Conclusions and future work** </br>s 
We proposed an extension to the state-of-the-art EDSR network by using it within a GAN framework. The proposed approach, EPSR, scales well in different regions of the perception-distortion plane and achieves superior perceptual scores when compared in a region-wise manner to other existing works. The performance improvement achieved by our approach is a cumulative result of the following factors: state-of-the-art SR network (EDSR) as the generator module, careful selection of loss function weights, and initialization of GAN training with the pretrained weights of EDSR. Our analysis of the perception-distortion tradeoff between BNet and EPSR signal the possibility to further boost the trade-off by adopting another generator module that yields better distortion measures. 
https://arxiv.org/pdf/1811.00344.pdf

 
**May 2019 - TGAN: Deep Tensor Generative Adversarial Nets for Large Image Generation**
<br>**Abstract**</br> 
Deep generative models have been successfully applied to many applications. However, existing works experience limitations when generating large images (the literature usually generates small images, e.g. 32 × 32 or 128 × 128). In this paper, we propose a novel scheme, called deep tensor adversarial generative nets (TGAN), that generates large high-quality images by exploring tensor structures. Essentially, the adversarial process of TGAN takes place in a tensor space. First, we impose tensor structures for concise image representation, which is superior in capturing the pixel proximity information and the spatial patterns of elementary objects in images, over the vectorization preprocess in existing works. Secondly, we propose TGAN that integrates deep convolutional generative adversarial networks and tensor super-resolution in a cascading manner, to generate high-quality images from random distributions. More specifically, we design a tensor super-resolution process that consists of tensor dictionary learning and tensor coefficients learning. Finally, on three datasets, the proposed TGAN generates images with more realistic textures, compared with state-of-the-art adversarial autoencoders. The size of the generated images is increased by over 8.5 times, namely 374 × 374 in PASCAL2.
<br>**Introduction** </br> 
With the great success in deep learning, the deep generative model have been investigated widely. The generative adversarial nets (GAN) [1] based methods are applied in many interesting applications including image super-resolution [2], image-to-image translation [3][4], text-to-image translation [5], dialogues generation [6], etc. With the development of graphical technologies, the demand of higher resolution images has increased significantly. Moreover, generation of large high-resolution images remains a challenge. However, existing GAN models experience limitations when generating large images. With the growing scale of images, vanilla GAN is hard to produce high-quality natural images because it is difficult for the generator and the discriminator to achieve optimality simultaneously. When processing high-dimensional images, the computation complexity and the training time increases significantly. The challenge is that the image has too many pixels and it is hard for a single generator G to learn the empirical distribution. Therefore, the traditional GAN [1] does not scale well for the generation of large images. The variations of GAN such as deep convolutional GAN (DCGAN) [7], super-resolution GAN (SRGAN) [8], Laplacian Pyramid GAN (LAPGAN) [9] and StackGAN [10] are promising candidates for generative models in unsupervised learning. It is desirable to construct a generative model that efficiently processes data with large size and high dimensions. Traditional GAN-based methods operates in pixel space to generate images while tensor-based methods work in tensor space. Tensor representation [11] and its derivative methods such as tensor sparse coding [12] and tensor superresolution have a more concise and efficient representation of images, especially for large images. They provide an alternative method for representing large images in the tensor space, instead of the traditional pixel space or frequency domain, which could benefit challenges of generating largesized high-resolution images. Large-sized or high-dimensional images can be realized in several possible ways. Super-resolution [13] is one of the classic methods used to construct high-resolution images from low-resolution images for better human interpretation. The key idea to achieve super-resolution is to use the nonredundant information contained in multiple lowresolution images induced by the subpixel shifts between them. One recent popular scheme for image super-resolution is SRGAN [8], which combines GAN with deep transposed convolutional neural networks (CNNs) for generating highresolution images from low-resolution ones. The generator in SRGAN is used for upsampling the low-resolution images to super-resolution images, which are distinguished from the original high-resolution images by the discriminator. Dictionary learning [14][15] is another method to efficiently to process large-sized or high-dimensional data. Using dictionary learning, we try to find sparse representation of input image data, which corresponds to the sparse coding technology of images. Traditional sparse coding method encodes images in matrices, while tensor-based sparse coding [12] is more flexible with larger representation space. Multi-dimensional tensor sparse coding uses t-linear combination to obtain a more concise and small dictionary for representing the images, and the corresponding coefficients have richer physical explanations than the traditional methods. We apply the basic principles of super-resolution and tensor-based dictionary learning in our generative model. For large-sized and high-dimensional images, the tensor representation is able to preserve the local proximity and capture the spatial patterns of elementary objects. Existing conventional sparse coding only captures linear correlations, which harms the spatial patterns of images. However, tensor sparse coding model can capture nonlinear correlations (linear upon sine/cosine basis), which is consistent with the existing neural networks using nonlinear activation functions. Tensor sparse coding replaces conventional vectorizing process with tensorizing process [16][12][17][18]. For complex and high-dimensional images, the conventional sparse coding process uses vector representation, and the vectorizing process ignores the spatial structure of the data. As a result, it generates a large-sized dictionary and causes high-computational complexity, which makes it infeasible for high-dimensional data applications. Tensor-based dictionary learning adopts a series of dictionaries to approximate the structures of the input data in each scale, which significantly reduces the size of the dictionaries. Besides, the circular matrix defined at Section 3.1 maintains the original image invariant after shifting; this helps to preserve the spatial structure of the images. Benefitting from tensor representation, tensor-based dictionary learning has advantages in dictionary size, shifting invariance, and rich physical explanations of the tensor coefficients [12]. In general, tensor-based methods have a more efficient representation capability for large-sized or high-dimensional data, and could therefore benefit the generative models. We believe that incorporating the tensor-based methods includig tensor representation, tensor sparse coding, and tensor superresolution in the generative models will improve large-sized high-resolution images generation. In this paper, we present a novel generative model called deep tensor generative adversarial nets (TGAN), cascading a DCGAN and tensor-based super-resolution to generate largesized high-quality images (e.g. 374×374). The contribution of the proposed TGAN is threefold: (i) We apply tensor representation and tensor sparse coding for images representation in generative models. This is testified to have advantages of more concise and efficient representation of images with less loss on spatial patterns. (ii) We incorporate the tensor representation into the super-resolution process, which is called tensor super-resolution. The tensor super-resolution is cascaded after a DCGAN with transposed convolutional layers, which generates low-resolution images directly from random distributions. (iii) The DCGAN and tensor dictionaries in tensor super-resolution are both pretrained with a large number of high-resolution and low-resolution images. The size of dictionaries is smaller with tensor representation than traditional, which accelerates the dictionary learning process in tensor super-resolution. More details are shown in Fig. 1 for an illustration of the TGAN. The generation performance of TGAN surpasses traditional generative models including adversarial autoencoders [19] in inception score [20] on test datasets, especially for large images. Our code is available at https://github.com/hust512/Tensor-GAN.
<br>**Conclusions and future work** </br> 
In this paper, we proposed a TGAN scheme that integrates DCGAN model and tensor super-resolution, which is able to generate large-sized high-quality images. The proposed scheme applies tensor representation space as main operation space for image generation, which shows better results than traditional generative models working in image pixel space. Essentially, the adversarial process of TGAN takes place in a tensor space. Note that in the tensor super-resolution process, tensor sparse coding brings several advantages: (i) the size of dictionary, which accelerates the training process for deriving the representation dictionary; (ii) more concise and efficient representation for images, which is verified in the generated images in our experiments. TGAN is superior in preserving spatial structures and local proximity information in images. Accordingly, the tensor super-resolution benefits from tensor representation to generate higher-quality images, especially for large images. Our proposed cascading TGAN scheme surpasses the state-of-the-art generative model AAE on three datasets (MNIST, CIFAR10, and PASCAL2).
https://arxiv.org/pdf/1901.09953.pdf




 
   **Jul 2019 - Deep Learning for Single Image Super-Resolution: A Brief Review**
<br>**Abstract**</br>
Single image super-resolution (SISR) is a notoriously challenging ill-posed problem that aims to obtain a highresolution (HR) output from one of its low-resolution (LR) versions. Recently, powerful deep learning algorithms have been applied to SISR and have achieved state-of-the-art performance. In this survey, we review representative deep learning-based SISR methods and group them into two categories according to their contributions to two essential aspects of SISR: the exploration of efficient neural network architectures for SISR and the development of effective optimization objectives for deep SISR learning. For each category, a baseline is first established, and several critical limitations of the baseline are summarized. Then, representative works on overcoming these limitations are presented based on their original content, as well as our critical exposition and analyses, and relevant comparisons are conducted from a variety of perspectives. Finally, we conclude this review with some current challenges and future trends in SISR that leverage deep learning algorithms. 
Index Terms—Single image super-resolution, deep learning, neural networks, objective function
<br>**Introduction** </br> 
DEEP learning (DL) [1] is a branch of machine learning algorithms that aims at learning the hierarchical representations of data. Deep learning has shown prominent superiority over other machine learning algorithms in many artificial intelligence domains, such as computer vision [2], speech recognition [3], and natural language processing [4]. Generally, the strong capacity of DL to address substantial unstructured data is attributable to two main contributors: the development of efficient computing hardware and the advancement of sophisticated algorithms. Single image super-resolution (SISR) is a notoriously challenging ill-posed problem because a specific low-resolution (LR) input can correspond to a crop of possible high-resolution (HR) images, and the HR space (in most instances, it refers to the natural image space) that we intend to map the LR input to is usually intractable [5]. Previous methods for SISR mainly have two drawbacks: one is the unclear definition of the mapping that we aim to develop between the LR space and the HR space, and the other is the inefficiency of establishing a complex high-dimensional mapping given massive raw data. Benefiting from the strong capacity of extracting effective high-level abstractions that bridge the LR and HR space, recent DL-based SISR methods have achieved significant improvements, both quantitatively and qualitatively. In this survey, we attempt to give an overall review of recent DL-based SISR algorithms. We mainly focus on two areas: efficient neural network architectures designed for SISR and effective optimization objectives for DL-based SISR learning. The reason for this taxonomy is that when we apply DL algorithms to tackle a specified task, it is best for us to consider both the universal DL strategies and the specific domain knowledge. From the perspective of DL, although many other techniques such as data preprocessing [6] and model training techniques are also quite important [7], [8], the combination of DL and domain knowledge in SISR is usually the key to success and is often reflected in the innovations of neural network architectures and optimization objectives for SISR. In each of these two focused areas, based on the benchmark, several representative works are discussed mainly from the perspective of their contributions and experimental results as well as our comments and views. The rest of the paper is arranged as follows. In Section II, we present relevant background concepts of SISR and DL. In Section III, we survey the literature on exploring efficient neural network architectures for various SISR tasks. In Section IV, we survey the studies on proposing effective objective functions for different purposes. In Section V, we summarize some trends and challenges for DL-based SISR. We conclude this survey in Section VI.
>> CONCLUSION
This paper presents a brief review of recent deep learning algorithms on SISR. It divides the recent works into two categories: the deep architectures for simulating the SISR process and the optimization objectives for optimizing the whole process. Despite the promising results reported so far, there are still many underlying problems. We summarize the main challenges into three aspects: the acceleration of deep models, the extensive comprehension of deep models and the criteria for designing and evaluating the objective functions. Along with these challenges, several directions may be further explored in the future.
https://arxiv.org/pdf/1808.03344.pdf
 
 **Apr 2022 - OUR-GAN: One-shot Ultra-high-Resolution Generative Adversarial Networks** 
<br>**Abstract**</br> 
We propose OUR-GAN, the first one-shot ultra-high-resolution (UHR) image synthesis framework that generates non-repetitive images with 4K or higher resolution from a single training image. OURGAN generates a visually coherent image at low resolution and then gradually increases the resolution by super-resolution. Since OUR-GAN learns from a real UHR image, it can synthesize large-scale shapes with fine details while maintaining long-range coherence, which is difficult with conventional generative models that generate large images based on the patch distribution learned from relatively small images. OUR-GAN applies seamless subregion-wise super-resolution that synthesizes 4k or higher UHR images with limited memory, preventing discontinuity at the boundary. Additionally, OUR-GAN improves visual coherence maintaining diversity by adding vertical positional embeddings to the feature maps. In experiments on the ST4K and RAISE datasets, OUR-GAN exhibited improved fidelity, visual coherency, and diversity compared with existing methods. The synthesized images are presented at https://anonymous-62348.github.io.
<br>**Introduction** </br> 
Recently, the demand for ultra-high-resolution (UHR) images with 4k or higher resolutions has increased. This paper proposes a framework one-shot ultra-high-resolution generative adversarial networks (OUR-GAN) that can synthesize a variety of high-quality UHR images from a single training image. Recently developed GAN models synthesize high-quality images by applying a variety of techniques such as the progressive growing approach, a hierarchy of multi-scale generators and discriminators, and auxiliary losses [1, 2, 3, 4, 5, 6, 7, 8, 9, 10]. However, the output resolution of most generative models is limited to 1K (1024x1024). Since a 4K UHD image consists of eight times more pixels than a 1K image, the UHR image synthesis model should synthesize substantially more information. To generate a UHR image with limited GPU memory, the model should synthesize images subregion by subregion and then concatenate them into a full-size image. However, it is difficult to maintain the visual coherence of global shape, the fidelity of detail, and shape diversity while synthesizing UHR images by subregion. There are only a few previous studies on non-repetitive UHR image synthesis with a limited amount of GPU memory [11, 12, 13]. They synthesize images of arbitrary size by concatenating patterns learned from small images. However, it is hard to generate images containing shapes larger than those of the training images in such a way, as shown in Fig. 2. In addition, they require numerous training samples, but collecting a large number of UHR images is costly and impossible in certain application fields. A few recently developed generative models are trainable with a single training image [8, 9, 14, 10, 15]. They generate diverse images with similar content and style to the training image using internal patch distribution learned from a single image. However, they cannot generate images with a resolution greater than 1K, and there is room for improvement in visual coherence and diversity. The proposed model, OUR-GAN, synthesizes non-repetitive high-fidelity UHR images with limited GPU memory and is trainable with a single training image, as shown in Fig. 1. Unlike existing models that generate UHR images by connecting small-scale patterns, OUR-GAN synthesizes diverse and visually coherent images at low resolution and then gradually increases the resolution by adding details via super-resolution. OUR-GAN synthesizes UHR images with limited GPU memory while preventing discontinuity at the boundary by a subregion-wise super-resolution. Since it learns directly from a real UHR image, it can generate large-scale shapes with fine details while maintaining long-range coherence. Additionally, we improved global shape coherence, maintaining diversity through vertical coordinate convolution. Up to our knowledge, OUR-GAN is the first non-repetitive UHR image synthesis model that is trainable with a single image. The main contributions of our study include 1) the first framework to synthesize non-repetitive high-fidelity UHR images from a single training image, 2) a seamless subregion-wise super-resolution method that synthesizes UHR images with limited GPU memory, 3) applying vertical coordinate convolution for improving the coherence of the global shape while maintaining diversity, 4) a new 4K dataset for one-shot UHR image synthesis, and 5) improved experimental results on ST4K and RAISE datasets in terms of fidelity, visual consistency, and diversity compared to conventional methods.
<br>**Conclusions and future work** </br> 
In this paper, we proposed the first One-shot Ultra-high-Resolution GAN framework (OUR-GAN) that synthesizes high-fidelity non-repetitive UHR images with a resolution of 4K or above and is learnable from a single image. OURGAN generates diverse and globally coherent large-scale shapes with fine details and maintains long-range coherence. We improved visual coherence by applying vertical coordinate convolution. With limited GPU memory, OUR-GAN can generate seamless UHR images with marginal overhead through the overlap-tile strategy combined with the proposed overlap estimation method.

https://arxiv.org/pdf/2202.13799.pdf
 
 **Mar 2023 - Synthesizing Realistic Image Restoration Training Pairs: A Diffusion Approach**
<br>**Abstract**</br> 
In supervised image restoration tasks, one key issue is how to obtain the aligned high-quality (HQ) and lowquality (LQ) training image pairs. Unfortunately, such HQLQ training pairs are hard to capture in practice, and hard to synthesize due to the complex unknown degradation in the wild. While several sophisticated degradation models have been manually designed to synthesize LQ images from their HQ counterparts, the distribution gap between the synthesized and real-world LQ images remains large. We propose a new approach to synthesizing realistic image restoration training pairs using the emerging denoising diffusion probabilistic model (DDPM). First, we train a DDPM, which could convert a noisy input into the desired LQ image, with a large amount of collected LQ images, which define the target data distribution. Then, for a given HQ image, we synthesize an initial LQ image by using an off-the-shelf degradation model, and iteratively add proper Gaussian noises to it. Finally, we denoise the noisy LQ image using the pre-trained DDPM to obtain the final LQ image, which falls into the target distribution of realworld LQ images. Thanks to the strong capability of DDPM in distribution approximation, the synthesized HQ-LQ image pairs can be used to train robust models for real-world image restoration tasks, such as blind face image restoration and blind image super-resolution. Experiments demonstrated the superiority of our proposed approach to existing degradation models. Code and data will be released. 
<br>**Introduction** </br> 
Deep neural networks (DNNs) [24] have been successfully used in a variety of computer vision tasks, including image classification [12], object detection [34], segmentation [11], as well as image restoration [6]. In supervised learning, the amount and quality of labeled training data will largely affect the practical performance of trained DNN models. This problem becomes more crucial in real-world image restoration tasks [51, 52], where the aligned high quality (HQ) and low-quality (LQ) training image pairs are very difficult to capture in practice. Early works mainly resort to using bicubic downsampling or some simple degradation models [23] to synthesize LQ images from their HQ counterparts, which however can only cover a very small set of degradation types. The real-world LQ images can suffer from many factors, including but not limited to low resolution, blur, noise, compression, etc., which are too complex to be explicitly modeled. As a result, the DNN models trained on synthesized HQ-LQ training pairs can hardly perform well on real-world LQ images. To alleviate the above difficulties, some works have been proposed to first predict the degradation parameters [9, 10, 18] and then restore the HQ image with them in a non-blind way. They work well on some specific non-blind deblurring [9], denoising [10] and JPEG artifacts removal [18] tasks. However, the degradation of real-world LQ images are often unknown and cannot be pre-defined, making it is hard, if not possible, to predict accurate degradation parameters. Some researchers attempted to collect realworld LQ-HQ pairs [2, 48] by using long-short camera focal lengths, which only work in applications where similar photographing devices are used. Recently, a couple of handcrafted degradation models [52, 45] have been proposed to model complex real-world degradations. Zhang et al. [52] randomly shuffled blur, downsampling and noise degradations to form a complex combination. Wang et al. [45] developed a high-order degradation model. While these methods can simulate a much larger scope of degradation types and have shown impressive progress in handling LQ images in the wild, the distribution gap between the synthesized and real-world LQ images remains large [26]. With the rapid advancement of deep generative networks [8, 20, 15], methods have been developed to learn how to synthesize LQ images from their HQ counterparts. Lugmayr et al. [29] learned a domain distribution network using unpaired data and then built HQ-LQ pairs with it. Similarly, Fritsche et al. [7] synthesized LQ images by using DSGAN to introduce natural image characteristics. Luo et al. [30] proposed a probabilistic degradation model (PDM) to describe different degradations. Most recently, Li et al. [26] developed the ReDegNet to model real-world degradations using paired face images and transfer them to produce LQ natural images. However, this method largely relies on the HQ faces generated by blind face restoration models [51, 44], which limits its applications. In this work, we revisit the problem of HQ-LQ image pair synthesis, which is critical to the many image restoration tasks such as blind face restoration and real-world image super-resolution. Our idea is to seamlessly integrate the advantages of handcrafted degradation models and deep generative networks. We first train a generator, i.e., the emerging denoising diffusion probabilistic model (DDPM) [15], by using a large amount of real-world LQ images collected from the Internet. The trained DDPM can be used to generate realistic LQ images. When building HQ-LQ image pairs, we first adopt a handcrafted degradation model to synthesize initial LQ images from the input HQ images, which may fall into the distribution of real-world LQ images. To reduce the distribution gap, the initially synthesized LQ images are added with Gaussian noise and then denoised by the pre-trained DDPM. After a few steps, the distribution of synthesized LQ images will become closer and closer to the distribution of real-world LQ images. Finally, a set of realistic HQ-LQ training pairs can be synthesized, and they can be used to train robust DNN models for image restoration tasks. In summary, in this work we present a novel diffusion approach to synthesizing realistic training pairs for image restoration, targeting at shortening the distribution gap between synthetic and real-world LQ data. The synthesized HQ-LQ training pairs can be used for various downstream real-world image restoration tasks, as validated in our experiments of blind image restoration and blind image superresolution. To the best of our knowledge, this is the first image degradation modeling method based on diffusion models. Codes will be made publicly available.

<br>**Conclusions and future work** </br> 
We proposed, for the first time to our best knowledge, to train a DDPM to synthesize realistic image restoration training pairs. With a collected LQ image dataset, a DDPM was first trained to approximate its distribution. The pretrained DDPM was then employed to convert the initially degraded image from its HQ counterpart into the desired LQ image, which fell into the distribution of real-world LQ images. With the synthesized realistic HQ-LQ image pairs, we re-trained the many state-of-the-art BFR and BISR models, and the re-trained models demonstrated much better realistic image restoration performance than their original counterparts both quantitatively and qualitatively. It should be noted that in our experiments, we collected a large scale degraded face dataset and a natural image dataset, respectively, as the target distributions to train the DDPM and synthesize HQ-LQ training pairs. In practice, the users can build their own LQ image dataset according to their needs, train the corresponding DDPM models and synthesize training pairs for different image restoration tasks.
https://arxiv.org/pdf/2303.06994.pdf

 

Bottom


[1]: <https://arxiv.org/pdf/1501.00092.pdf> "Generative Adversarial Nets (Jun 2014)"

[2]: <https://arxiv.org/pdf/1511.06434.pdf> "Unsupervised Representation Learning With Deep Convolutional Generative Adversarial Networks (Jan 2016)"

[3]: <https://arxiv.org/pdf/1501.00092.pdf> "Image Super-Resolution Using Deep Convolutional Networks (Jul 2015)"

[4]: <https://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&arnumber=7780551> "Accurate Image Super-Resolution Using Very Deep Convolutional Networks (2016)"

[5]: <https://arxiv.org/pdf/1511.04491.pdf> "Deeply-Recursive Convolutional Network for Image Super-Resolution (Nov 2016)"

[6]: <https://arxiv.org/pdf/1609.04802.pdf> "Photo-Realistic Single Image Super-Resolution Using a Generative Adversarial Network (May 2017)"

[7]: <https://arxiv.org/pdf/1707.00737.pdf> "High-Quality Face Image Super-Resolution Using Conditional Generative Adversarial Networks (Jul 2017)"

[8]: <https://ieeexplore-ieee-org.ezproxy.is.ed.ac.uk/stamp/stamp.jsp?tp=&arnumber=8578518> "Feature Super-Resolution: Make Machine See More Clearly (2018)"

[9]: <https://arxiv.org/pdf/1809.00219.pdf> "ESRGAN: Enhanced Super-Resolution Generative Adversarial Networks (Sep 2018)"

[10]: <https://arxiv.org/pdf/1811.00344.pdf> "Analyzing Perception-Distortion Tradeoff using Enhanced Perceptual Super-resolution Network (Nov 2018)"

[11]: <https://arxiv.org/pdf/1901.09953.pdf> "TGAN: Deep Tensor Generative Adversarial Nets for Large Image Generation (May 2019)"

[12]: <https://arxiv.org/pdf/1808.03344.pdf> "Deep Learning for Single Image Super-Resolution: A Brief Review (Jul 2019)"

[13]: <https://arxiv.org/pdf/2202.13799.pdf> "OUR-GAN: One-shot Ultra-high-Resolution Generative Adversarial Networks (Apr 2022)"

[14]: <https://arxiv.org/pdf/2303.06994.pdf> "Synthesizing Realistic Image Restoration Training Pairs: A Diffusion Approach (Mar 2023)"

